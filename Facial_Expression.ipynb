{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial_Expression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsc7infM0Y9J",
        "colab_type": "code",
        "outputId": "ae0a832c-33d1-4d6d-fc07-ba87101ebe20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H31PxWC3Get",
        "colab_type": "code",
        "outputId": "1c382ffb-445b-4f55-a28d-cf3c4e58137f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten,Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "from keras.models import load_model,Model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1seO7vbP33m2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(\"/content/drive/My Drive/FER/fer2013.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medeYUTY5b7A",
        "colab_type": "code",
        "outputId": "278ca8f0-5750-4fbd-a5f3-8e656a3feff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYfZXOpa5gFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,y_train,X_test,y_test=[],[],[],[]\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    val=row['pixels'].split(\" \")\n",
        "    try:\n",
        "        if 'Training' in row['Usage']:\n",
        "           X_train.append(np.array(val,'float32'))\n",
        "           y_train.append(row['emotion'])\n",
        "        elif 'PublicTest' in row['Usage']:\n",
        "           X_test.append(np.array(val,'float32'))\n",
        "           y_test.append(row['emotion'])\n",
        "    except:\n",
        "        print(f\"error occured at index :{index} and row:{row}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE1nZpiSfVsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 64\n",
        "num_labels = 7\n",
        "width, height = 48, 48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnIUOu_Zc-ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array(X_train,'float32')\n",
        "y_train = np.array(y_train,'float32')\n",
        "X_test = np.array(X_test,'float32')\n",
        "y_test = np.array(y_test,'float32')\n",
        "\n",
        "y_train=np_utils.to_categorical(y_train, num_classes=num_labels)\n",
        "y_test=np_utils.to_categorical(y_test, num_classes=num_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyFePCvse-CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalizing data between o and 1\n",
        "X_train -= np.mean(X_train, axis=0)\n",
        "X_train /= np.std(X_train, axis=0)\n",
        "\n",
        "X_test -= np.mean(X_test, axis=0)\n",
        "X_test /= np.std(X_test, axis=0)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], 48 ,48, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvpZP9PgCUk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "d1540ef1-f2fa-4e11-ac9c-70cb3d601f27"
      },
      "source": [
        "# Architecture of the CNN Model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolution layer\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\n",
        "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# 2nd Convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# 3rd Convolution layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully Connected layer\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 46, 46, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 44, 44, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 20, 20, 64)        36928     \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 18, 18, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 9, 9, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 5, 5, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 7)                 7175      \n",
            "=================================================================\n",
            "Total params: 1,919,303\n",
            "Trainable params: 1,917,127\n",
            "Non-trainable params: 2,176\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7IIEqYfgZbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compliling the model\n",
        "model.compile(loss=categorical_crossentropy,optimizer=Adam(),metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5JX3kEEg8eG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cab9450-2987-4f47-bc2a-72abf9f5bf7e"
      },
      "source": [
        "#Training the model\n",
        "model.fit(X_train, y_train,batch_size=256,epochs=200,verbose=1, validation_data=(X_test, y_test),shuffle=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/200\n",
            "28709/28709 [==============================] - 10s 336us/step - loss: 2.1894 - acc: 0.2170 - val_loss: 1.8193 - val_acc: 0.2494\n",
            "Epoch 2/200\n",
            "28709/28709 [==============================] - 8s 284us/step - loss: 1.9576 - acc: 0.2380 - val_loss: 1.8114 - val_acc: 0.2491\n",
            "Epoch 3/200\n",
            "28709/28709 [==============================] - 8s 286us/step - loss: 1.8940 - acc: 0.2515 - val_loss: 1.8120 - val_acc: 0.2516\n",
            "Epoch 4/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.8757 - acc: 0.2505 - val_loss: 1.8650 - val_acc: 0.2555\n",
            "Epoch 5/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.8517 - acc: 0.2669 - val_loss: 1.8708 - val_acc: 0.2536\n",
            "Epoch 6/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 1.8128 - acc: 0.2894 - val_loss: 1.7213 - val_acc: 0.2881\n",
            "Epoch 7/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 1.8162 - acc: 0.2989 - val_loss: 1.8279 - val_acc: 0.3257\n",
            "Epoch 8/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 1.7655 - acc: 0.3190 - val_loss: 1.6518 - val_acc: 0.3199\n",
            "Epoch 9/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.6953 - acc: 0.3415 - val_loss: 1.7366 - val_acc: 0.3054\n",
            "Epoch 10/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.6747 - acc: 0.3596 - val_loss: 1.8804 - val_acc: 0.2962\n",
            "Epoch 11/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.6480 - acc: 0.3726 - val_loss: 1.5096 - val_acc: 0.4115\n",
            "Epoch 12/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.6239 - acc: 0.3805 - val_loss: 1.4927 - val_acc: 0.4302\n",
            "Epoch 13/200\n",
            "28709/28709 [==============================] - 8s 288us/step - loss: 1.5963 - acc: 0.3890 - val_loss: 1.5953 - val_acc: 0.3622\n",
            "Epoch 14/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.5905 - acc: 0.3999 - val_loss: 1.5370 - val_acc: 0.4383\n",
            "Epoch 15/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.5733 - acc: 0.4044 - val_loss: 1.4536 - val_acc: 0.4472\n",
            "Epoch 16/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.5354 - acc: 0.4206 - val_loss: 1.4208 - val_acc: 0.4483\n",
            "Epoch 17/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.5161 - acc: 0.4239 - val_loss: 1.3949 - val_acc: 0.4648\n",
            "Epoch 18/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.5044 - acc: 0.4346 - val_loss: 1.3866 - val_acc: 0.4617\n",
            "Epoch 19/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.4804 - acc: 0.4436 - val_loss: 1.3619 - val_acc: 0.4792\n",
            "Epoch 20/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.4711 - acc: 0.4400 - val_loss: 1.3939 - val_acc: 0.4726\n",
            "Epoch 21/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.4453 - acc: 0.4469 - val_loss: 1.3331 - val_acc: 0.4971\n",
            "Epoch 22/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.4096 - acc: 0.4574 - val_loss: 1.3383 - val_acc: 0.4868\n",
            "Epoch 23/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.3958 - acc: 0.4622 - val_loss: 1.3173 - val_acc: 0.4982\n",
            "Epoch 24/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.3756 - acc: 0.4688 - val_loss: 1.2901 - val_acc: 0.5102\n",
            "Epoch 25/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.3577 - acc: 0.4771 - val_loss: 1.2868 - val_acc: 0.4993\n",
            "Epoch 26/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.3464 - acc: 0.4815 - val_loss: 1.2667 - val_acc: 0.5135\n",
            "Epoch 27/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.3341 - acc: 0.4872 - val_loss: 1.2805 - val_acc: 0.5013\n",
            "Epoch 28/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.3303 - acc: 0.4900 - val_loss: 1.2263 - val_acc: 0.5249\n",
            "Epoch 29/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.3180 - acc: 0.4927 - val_loss: 1.2446 - val_acc: 0.5294\n",
            "Epoch 30/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.3011 - acc: 0.5005 - val_loss: 1.2425 - val_acc: 0.5235\n",
            "Epoch 31/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.2984 - acc: 0.5022 - val_loss: 1.2024 - val_acc: 0.5380\n",
            "Epoch 32/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.2872 - acc: 0.5062 - val_loss: 1.2189 - val_acc: 0.5439\n",
            "Epoch 33/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.2825 - acc: 0.5099 - val_loss: 1.1948 - val_acc: 0.5366\n",
            "Epoch 34/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.2671 - acc: 0.5158 - val_loss: 1.2019 - val_acc: 0.5366\n",
            "Epoch 35/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.2590 - acc: 0.5191 - val_loss: 1.2045 - val_acc: 0.5436\n",
            "Epoch 36/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.2551 - acc: 0.5223 - val_loss: 1.1921 - val_acc: 0.5481\n",
            "Epoch 37/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.2512 - acc: 0.5208 - val_loss: 1.2112 - val_acc: 0.5400\n",
            "Epoch 38/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.2438 - acc: 0.5276 - val_loss: 1.1673 - val_acc: 0.5492\n",
            "Epoch 39/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.2315 - acc: 0.5298 - val_loss: 1.2101 - val_acc: 0.5391\n",
            "Epoch 40/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.2236 - acc: 0.5356 - val_loss: 1.1618 - val_acc: 0.5525\n",
            "Epoch 41/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.2270 - acc: 0.5329 - val_loss: 1.1834 - val_acc: 0.5478\n",
            "Epoch 42/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.2238 - acc: 0.5339 - val_loss: 1.1746 - val_acc: 0.5536\n",
            "Epoch 43/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.2092 - acc: 0.5431 - val_loss: 1.1608 - val_acc: 0.5556\n",
            "Epoch 44/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.2068 - acc: 0.5430 - val_loss: 1.1565 - val_acc: 0.5648\n",
            "Epoch 45/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.2028 - acc: 0.5437 - val_loss: 1.1591 - val_acc: 0.5595\n",
            "Epoch 46/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1995 - acc: 0.5458 - val_loss: 1.1665 - val_acc: 0.5573\n",
            "Epoch 47/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.1888 - acc: 0.5466 - val_loss: 1.1565 - val_acc: 0.5626\n",
            "Epoch 48/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.1939 - acc: 0.5476 - val_loss: 1.1423 - val_acc: 0.5656\n",
            "Epoch 49/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1839 - acc: 0.5492 - val_loss: 1.1392 - val_acc: 0.5623\n",
            "Epoch 50/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.1795 - acc: 0.5527 - val_loss: 1.1535 - val_acc: 0.5639\n",
            "Epoch 51/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1755 - acc: 0.5541 - val_loss: 1.1819 - val_acc: 0.5475\n",
            "Epoch 52/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1705 - acc: 0.5539 - val_loss: 1.1294 - val_acc: 0.5715\n",
            "Epoch 53/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1657 - acc: 0.5584 - val_loss: 1.1570 - val_acc: 0.5651\n",
            "Epoch 54/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1589 - acc: 0.5594 - val_loss: 1.1315 - val_acc: 0.5701\n",
            "Epoch 55/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1581 - acc: 0.5594 - val_loss: 1.1411 - val_acc: 0.5709\n",
            "Epoch 56/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.1551 - acc: 0.5626 - val_loss: 1.1198 - val_acc: 0.5762\n",
            "Epoch 57/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1502 - acc: 0.5641 - val_loss: 1.1312 - val_acc: 0.5723\n",
            "Epoch 58/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1429 - acc: 0.5644 - val_loss: 1.1325 - val_acc: 0.5720\n",
            "Epoch 59/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.1321 - acc: 0.5733 - val_loss: 1.1200 - val_acc: 0.5801\n",
            "Epoch 60/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1353 - acc: 0.5686 - val_loss: 1.1353 - val_acc: 0.5729\n",
            "Epoch 61/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1348 - acc: 0.5679 - val_loss: 1.0975 - val_acc: 0.5851\n",
            "Epoch 62/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1259 - acc: 0.5754 - val_loss: 1.1231 - val_acc: 0.5734\n",
            "Epoch 63/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1250 - acc: 0.5729 - val_loss: 1.0957 - val_acc: 0.5843\n",
            "Epoch 64/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.1254 - acc: 0.5739 - val_loss: 1.1052 - val_acc: 0.5776\n",
            "Epoch 65/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1239 - acc: 0.5751 - val_loss: 1.1040 - val_acc: 0.5804\n",
            "Epoch 66/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.1272 - acc: 0.5714 - val_loss: 1.1560 - val_acc: 0.5536\n",
            "Epoch 67/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1143 - acc: 0.5771 - val_loss: 1.1036 - val_acc: 0.5846\n",
            "Epoch 68/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1121 - acc: 0.5803 - val_loss: 1.1022 - val_acc: 0.5782\n",
            "Epoch 69/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1099 - acc: 0.5785 - val_loss: 1.0831 - val_acc: 0.5940\n",
            "Epoch 70/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.1087 - acc: 0.5785 - val_loss: 1.0889 - val_acc: 0.5876\n",
            "Epoch 71/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.1040 - acc: 0.5847 - val_loss: 1.1091 - val_acc: 0.5821\n",
            "Epoch 72/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.0972 - acc: 0.5847 - val_loss: 1.0866 - val_acc: 0.5915\n",
            "Epoch 73/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0977 - acc: 0.5870 - val_loss: 1.0971 - val_acc: 0.5910\n",
            "Epoch 74/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0946 - acc: 0.5872 - val_loss: 1.0909 - val_acc: 0.5896\n",
            "Epoch 75/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0868 - acc: 0.5896 - val_loss: 1.0946 - val_acc: 0.5726\n",
            "Epoch 76/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0908 - acc: 0.5873 - val_loss: 1.0910 - val_acc: 0.5829\n",
            "Epoch 77/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0919 - acc: 0.5862 - val_loss: 1.0862 - val_acc: 0.5913\n",
            "Epoch 78/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0871 - acc: 0.5874 - val_loss: 1.0943 - val_acc: 0.5893\n",
            "Epoch 79/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 1.0826 - acc: 0.5909 - val_loss: 1.0915 - val_acc: 0.5876\n",
            "Epoch 80/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0749 - acc: 0.5922 - val_loss: 1.0820 - val_acc: 0.5988\n",
            "Epoch 81/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.0819 - acc: 0.5937 - val_loss: 1.0801 - val_acc: 0.5952\n",
            "Epoch 82/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0733 - acc: 0.5928 - val_loss: 1.0764 - val_acc: 0.5915\n",
            "Epoch 83/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.0745 - acc: 0.5924 - val_loss: 1.1120 - val_acc: 0.5854\n",
            "Epoch 84/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0736 - acc: 0.5958 - val_loss: 1.1168 - val_acc: 0.5801\n",
            "Epoch 85/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0670 - acc: 0.5972 - val_loss: 1.0820 - val_acc: 0.5896\n",
            "Epoch 86/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0639 - acc: 0.5961 - val_loss: 1.0740 - val_acc: 0.5938\n",
            "Epoch 87/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0598 - acc: 0.6004 - val_loss: 1.0641 - val_acc: 0.5991\n",
            "Epoch 88/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0517 - acc: 0.6008 - val_loss: 1.0729 - val_acc: 0.5940\n",
            "Epoch 89/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0573 - acc: 0.6000 - val_loss: 1.0757 - val_acc: 0.5896\n",
            "Epoch 90/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0520 - acc: 0.6005 - val_loss: 1.0580 - val_acc: 0.6043\n",
            "Epoch 91/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0571 - acc: 0.6024 - val_loss: 1.0645 - val_acc: 0.6024\n",
            "Epoch 92/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0496 - acc: 0.6062 - val_loss: 1.0813 - val_acc: 0.5949\n",
            "Epoch 93/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0559 - acc: 0.6009 - val_loss: 1.0961 - val_acc: 0.5893\n",
            "Epoch 94/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0464 - acc: 0.6033 - val_loss: 1.0817 - val_acc: 0.5915\n",
            "Epoch 95/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0468 - acc: 0.6030 - val_loss: 1.0588 - val_acc: 0.5940\n",
            "Epoch 96/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0468 - acc: 0.6035 - val_loss: 1.0590 - val_acc: 0.5993\n",
            "Epoch 97/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.0440 - acc: 0.6018 - val_loss: 1.0727 - val_acc: 0.5991\n",
            "Epoch 98/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0479 - acc: 0.6050 - val_loss: 1.0496 - val_acc: 0.6124\n",
            "Epoch 99/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0401 - acc: 0.6074 - val_loss: 1.0690 - val_acc: 0.6027\n",
            "Epoch 100/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0362 - acc: 0.6117 - val_loss: 1.0681 - val_acc: 0.6043\n",
            "Epoch 101/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0341 - acc: 0.6103 - val_loss: 1.0693 - val_acc: 0.6055\n",
            "Epoch 102/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0353 - acc: 0.6090 - val_loss: 1.0565 - val_acc: 0.6057\n",
            "Epoch 103/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0272 - acc: 0.6075 - val_loss: 1.0591 - val_acc: 0.5999\n",
            "Epoch 104/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0294 - acc: 0.6108 - val_loss: 1.0645 - val_acc: 0.6080\n",
            "Epoch 105/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0274 - acc: 0.6089 - val_loss: 1.0647 - val_acc: 0.6004\n",
            "Epoch 106/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0240 - acc: 0.6110 - val_loss: 1.0651 - val_acc: 0.5943\n",
            "Epoch 107/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0254 - acc: 0.6139 - val_loss: 1.0537 - val_acc: 0.6119\n",
            "Epoch 108/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0257 - acc: 0.6159 - val_loss: 1.0561 - val_acc: 0.6049\n",
            "Epoch 109/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0219 - acc: 0.6159 - val_loss: 1.0573 - val_acc: 0.6007\n",
            "Epoch 110/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0212 - acc: 0.6153 - val_loss: 1.0546 - val_acc: 0.6043\n",
            "Epoch 111/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0170 - acc: 0.6161 - val_loss: 1.0547 - val_acc: 0.6066\n",
            "Epoch 112/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0155 - acc: 0.6194 - val_loss: 1.0467 - val_acc: 0.5996\n",
            "Epoch 113/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.0153 - acc: 0.6184 - val_loss: 1.0449 - val_acc: 0.6052\n",
            "Epoch 114/200\n",
            "28709/28709 [==============================] - 8s 288us/step - loss: 1.0014 - acc: 0.6249 - val_loss: 1.0481 - val_acc: 0.6049\n",
            "Epoch 115/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0081 - acc: 0.6207 - val_loss: 1.0459 - val_acc: 0.6049\n",
            "Epoch 116/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0027 - acc: 0.6201 - val_loss: 1.0357 - val_acc: 0.6124\n",
            "Epoch 117/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.0043 - acc: 0.6190 - val_loss: 1.0415 - val_acc: 0.6074\n",
            "Epoch 118/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0025 - acc: 0.6191 - val_loss: 1.0445 - val_acc: 0.6152\n",
            "Epoch 119/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 1.0041 - acc: 0.6219 - val_loss: 1.0569 - val_acc: 0.6041\n",
            "Epoch 120/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 1.0012 - acc: 0.6228 - val_loss: 1.0397 - val_acc: 0.6080\n",
            "Epoch 121/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 1.0037 - acc: 0.6220 - val_loss: 1.0456 - val_acc: 0.6099\n",
            "Epoch 122/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9970 - acc: 0.6256 - val_loss: 1.0500 - val_acc: 0.6041\n",
            "Epoch 123/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9954 - acc: 0.6223 - val_loss: 1.0611 - val_acc: 0.5993\n",
            "Epoch 124/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 0.9935 - acc: 0.6252 - val_loss: 1.0500 - val_acc: 0.6071\n",
            "Epoch 125/200\n",
            "28709/28709 [==============================] - 8s 293us/step - loss: 0.9930 - acc: 0.6268 - val_loss: 1.0450 - val_acc: 0.6102\n",
            "Epoch 126/200\n",
            "28709/28709 [==============================] - 8s 293us/step - loss: 0.9955 - acc: 0.6228 - val_loss: 1.0405 - val_acc: 0.6049\n",
            "Epoch 127/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 0.9860 - acc: 0.6276 - val_loss: 1.0428 - val_acc: 0.6152\n",
            "Epoch 128/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 0.9853 - acc: 0.6303 - val_loss: 1.0429 - val_acc: 0.6088\n",
            "Epoch 129/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9890 - acc: 0.6288 - val_loss: 1.0508 - val_acc: 0.6110\n",
            "Epoch 130/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 0.9901 - acc: 0.6273 - val_loss: 1.0426 - val_acc: 0.6119\n",
            "Epoch 131/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9928 - acc: 0.6280 - val_loss: 1.0497 - val_acc: 0.6030\n",
            "Epoch 132/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 0.9826 - acc: 0.6285 - val_loss: 1.0640 - val_acc: 0.6049\n",
            "Epoch 133/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 0.9798 - acc: 0.6312 - val_loss: 1.0475 - val_acc: 0.6096\n",
            "Epoch 134/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 0.9760 - acc: 0.6294 - val_loss: 1.0438 - val_acc: 0.6069\n",
            "Epoch 135/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9869 - acc: 0.6288 - val_loss: 1.0469 - val_acc: 0.6052\n",
            "Epoch 136/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9835 - acc: 0.6264 - val_loss: 1.0375 - val_acc: 0.6141\n",
            "Epoch 137/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9761 - acc: 0.6314 - val_loss: 1.0373 - val_acc: 0.6102\n",
            "Epoch 138/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 0.9720 - acc: 0.6334 - val_loss: 1.0509 - val_acc: 0.6002\n",
            "Epoch 139/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 0.9701 - acc: 0.6354 - val_loss: 1.0421 - val_acc: 0.6077\n",
            "Epoch 140/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9769 - acc: 0.6291 - val_loss: 1.0399 - val_acc: 0.6121\n",
            "Epoch 141/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9655 - acc: 0.6352 - val_loss: 1.0498 - val_acc: 0.6082\n",
            "Epoch 142/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9647 - acc: 0.6368 - val_loss: 1.0452 - val_acc: 0.6110\n",
            "Epoch 143/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9713 - acc: 0.6344 - val_loss: 1.0381 - val_acc: 0.6133\n",
            "Epoch 144/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9632 - acc: 0.6361 - val_loss: 1.0477 - val_acc: 0.6091\n",
            "Epoch 145/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 0.9638 - acc: 0.6350 - val_loss: 1.0372 - val_acc: 0.6108\n",
            "Epoch 146/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9648 - acc: 0.6381 - val_loss: 1.0373 - val_acc: 0.6096\n",
            "Epoch 147/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 0.9632 - acc: 0.6361 - val_loss: 1.0289 - val_acc: 0.6158\n",
            "Epoch 148/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9647 - acc: 0.6359 - val_loss: 1.0388 - val_acc: 0.6066\n",
            "Epoch 149/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9540 - acc: 0.6386 - val_loss: 1.0335 - val_acc: 0.6135\n",
            "Epoch 150/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9563 - acc: 0.6398 - val_loss: 1.0444 - val_acc: 0.6082\n",
            "Epoch 151/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9554 - acc: 0.6397 - val_loss: 1.0435 - val_acc: 0.6066\n",
            "Epoch 152/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9594 - acc: 0.6399 - val_loss: 1.0345 - val_acc: 0.6121\n",
            "Epoch 153/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 0.9566 - acc: 0.6392 - val_loss: 1.0333 - val_acc: 0.6119\n",
            "Epoch 154/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 0.9509 - acc: 0.6436 - val_loss: 1.0301 - val_acc: 0.6108\n",
            "Epoch 155/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 0.9508 - acc: 0.6419 - val_loss: 1.0492 - val_acc: 0.6110\n",
            "Epoch 156/200\n",
            "28709/28709 [==============================] - 8s 291us/step - loss: 0.9480 - acc: 0.6436 - val_loss: 1.0314 - val_acc: 0.6096\n",
            "Epoch 157/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 0.9561 - acc: 0.6389 - val_loss: 1.0485 - val_acc: 0.6046\n",
            "Epoch 158/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9532 - acc: 0.6388 - val_loss: 1.0504 - val_acc: 0.6066\n",
            "Epoch 159/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9438 - acc: 0.6416 - val_loss: 1.0368 - val_acc: 0.6088\n",
            "Epoch 160/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9508 - acc: 0.6400 - val_loss: 1.0407 - val_acc: 0.6110\n",
            "Epoch 161/200\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 0.9480 - acc: 0.6462 - val_loss: 1.0350 - val_acc: 0.6127\n",
            "Epoch 162/200\n",
            "28709/28709 [==============================] - 8s 290us/step - loss: 0.9434 - acc: 0.6436 - val_loss: 1.0334 - val_acc: 0.6144\n",
            "Epoch 163/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9435 - acc: 0.6461 - val_loss: 1.0402 - val_acc: 0.6074\n",
            "Epoch 164/200\n",
            "28709/28709 [==============================] - 8s 296us/step - loss: 0.9411 - acc: 0.6458 - val_loss: 1.0253 - val_acc: 0.6152\n",
            "Epoch 165/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9389 - acc: 0.6484 - val_loss: 1.0335 - val_acc: 0.6113\n",
            "Epoch 166/200\n",
            "28709/28709 [==============================] - 9s 297us/step - loss: 0.9436 - acc: 0.6432 - val_loss: 1.0247 - val_acc: 0.6191\n",
            "Epoch 167/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9415 - acc: 0.6483 - val_loss: 1.0180 - val_acc: 0.6183\n",
            "Epoch 168/200\n",
            "28709/28709 [==============================] - 9s 296us/step - loss: 0.9448 - acc: 0.6449 - val_loss: 1.0293 - val_acc: 0.6149\n",
            "Epoch 169/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9324 - acc: 0.6461 - val_loss: 1.0455 - val_acc: 0.6074\n",
            "Epoch 170/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9325 - acc: 0.6507 - val_loss: 1.0424 - val_acc: 0.6141\n",
            "Epoch 171/200\n",
            "28709/28709 [==============================] - 8s 296us/step - loss: 0.9387 - acc: 0.6465 - val_loss: 1.0355 - val_acc: 0.6166\n",
            "Epoch 172/200\n",
            "28709/28709 [==============================] - 8s 296us/step - loss: 0.9303 - acc: 0.6506 - val_loss: 1.0165 - val_acc: 0.6174\n",
            "Epoch 173/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9358 - acc: 0.6467 - val_loss: 1.0569 - val_acc: 0.6080\n",
            "Epoch 174/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9339 - acc: 0.6468 - val_loss: 1.0233 - val_acc: 0.6133\n",
            "Epoch 175/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9341 - acc: 0.6480 - val_loss: 1.0345 - val_acc: 0.6096\n",
            "Epoch 176/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9325 - acc: 0.6515 - val_loss: 1.0488 - val_acc: 0.6155\n",
            "Epoch 177/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9335 - acc: 0.6506 - val_loss: 1.0273 - val_acc: 0.6177\n",
            "Epoch 178/200\n",
            "28709/28709 [==============================] - 8s 296us/step - loss: 0.9293 - acc: 0.6532 - val_loss: 1.0400 - val_acc: 0.6102\n",
            "Epoch 179/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9295 - acc: 0.6513 - val_loss: 1.0221 - val_acc: 0.6160\n",
            "Epoch 180/200\n",
            "28709/28709 [==============================] - 9s 297us/step - loss: 0.9275 - acc: 0.6521 - val_loss: 1.0340 - val_acc: 0.6135\n",
            "Epoch 181/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9221 - acc: 0.6535 - val_loss: 1.0299 - val_acc: 0.6124\n",
            "Epoch 182/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9193 - acc: 0.6561 - val_loss: 1.0280 - val_acc: 0.6194\n",
            "Epoch 183/200\n",
            "28709/28709 [==============================] - 8s 296us/step - loss: 0.9219 - acc: 0.6524 - val_loss: 1.0329 - val_acc: 0.6127\n",
            "Epoch 184/200\n",
            "28709/28709 [==============================] - 8s 296us/step - loss: 0.9218 - acc: 0.6517 - val_loss: 1.0252 - val_acc: 0.6183\n",
            "Epoch 185/200\n",
            "28709/28709 [==============================] - 8s 296us/step - loss: 0.9210 - acc: 0.6546 - val_loss: 1.0357 - val_acc: 0.6149\n",
            "Epoch 186/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9203 - acc: 0.6529 - val_loss: 1.0477 - val_acc: 0.6063\n",
            "Epoch 187/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9182 - acc: 0.6540 - val_loss: 1.0305 - val_acc: 0.6163\n",
            "Epoch 188/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9199 - acc: 0.6522 - val_loss: 1.0383 - val_acc: 0.6149\n",
            "Epoch 189/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9162 - acc: 0.6579 - val_loss: 1.0176 - val_acc: 0.6255\n",
            "Epoch 190/200\n",
            "28709/28709 [==============================] - 8s 295us/step - loss: 0.9164 - acc: 0.6565 - val_loss: 1.0422 - val_acc: 0.6110\n",
            "Epoch 191/200\n",
            "28709/28709 [==============================] - 9s 296us/step - loss: 0.9197 - acc: 0.6515 - val_loss: 1.0259 - val_acc: 0.6119\n",
            "Epoch 192/200\n",
            "28709/28709 [==============================] - 8s 296us/step - loss: 0.9084 - acc: 0.6566 - val_loss: 1.0191 - val_acc: 0.6169\n",
            "Epoch 193/200\n",
            "28709/28709 [==============================] - 8s 293us/step - loss: 0.9079 - acc: 0.6583 - val_loss: 1.0257 - val_acc: 0.6088\n",
            "Epoch 194/200\n",
            "28709/28709 [==============================] - 8s 293us/step - loss: 0.9109 - acc: 0.6569 - val_loss: 1.0297 - val_acc: 0.6119\n",
            "Epoch 195/200\n",
            "28709/28709 [==============================] - 8s 294us/step - loss: 0.9097 - acc: 0.6544 - val_loss: 1.0235 - val_acc: 0.6247\n",
            "Epoch 196/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 0.9136 - acc: 0.6560 - val_loss: 1.0214 - val_acc: 0.6166\n",
            "Epoch 197/200\n",
            "28709/28709 [==============================] - 8s 293us/step - loss: 0.9099 - acc: 0.6551 - val_loss: 1.0333 - val_acc: 0.6202\n",
            "Epoch 198/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 0.9114 - acc: 0.6578 - val_loss: 1.0202 - val_acc: 0.6194\n",
            "Epoch 199/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 0.9086 - acc: 0.6579 - val_loss: 1.0195 - val_acc: 0.6169\n",
            "Epoch 200/200\n",
            "28709/28709 [==============================] - 8s 292us/step - loss: 0.9082 - acc: 0.6625 - val_loss: 1.0586 - val_acc: 0.6002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f15f9f4f550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziRrDm0d0hcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"/content/drive/My Drive/Facial_Emotion_Detector.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qxyh9XSNAsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the  model to  use it later on\n",
        "fer_json = model.to_json()\n",
        "with open(\"/content/drive/My Drive/Emotion_Detector_Model.json\", \"w\") as json_file:\n",
        "    json_file.write(fer_json)\n",
        "model.save_weights(\"/content/drive/My Drive/Emotion_Detector_Model_Weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}